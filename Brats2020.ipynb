{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.11",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kaggle": {
            "accelerator": "nvidiaTeslaT4",
            "dataSources": [
                {
                    "sourceId": 1299795,
                    "sourceType": "datasetVersion",
                    "datasetId": 751906
                }
            ],
            "dockerImageVersionId": 31011,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook",
            "isGpuEnabled": true
        }
    },
    "nbformat_minor": 4,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "code",
            "source": "import os\nimport torch\nimport logging\n\nimport sys\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nfrom matplotlib import cm\nimport matplotlib.animation as anim\nimport matplotlib.patches as mpatches\nimport matplotlib.gridspec as gridspec\n\nimport nibabel as nib\nimport torch.nn as nn\nimport torch.optim as optim\nimport gc\nimport torch\n\nimport torch.nn.functional as F\nimport os\nimport numpy as np\nimport nibabel as nib\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\nfrom tqdm import tqdm\n\nfrom torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.cuda.amp import autocast, GradScaler\nfrom sklearn.metrics import confusion_matrix\nimport kagglehub\nfrom huggingface_hub import HfApi, create_repo\nimport os\nfrom datetime import datetime\nfrom kaggle_secrets import UserSecretsClient\nimport cv2",
            "metadata": {
                "id": "H7a3q0uFV6KS",
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:32:33.236654Z",
                    "iopub.execute_input": "2025-07-01T17:32:33.236952Z",
                    "iopub.status.idle": "2025-07-01T17:33:06.735533Z",
                    "shell.execute_reply.started": "2025-07-01T17:32:33.236931Z",
                    "shell.execute_reply": "2025-07-01T17:33:06.734974Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "text": "2025-07-01 17:32:50.320460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751391170.724829      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751391170.845427      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 1
        },
        {
            "cell_type": "markdown",
            "source": "## Model creation",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "class ImprovedUNet3D(nn.Module):\n    def __init__(self, in_channels=4, out_channels=4, base_filters=16):\n        super(ImprovedUNet3D, self).__init__()\n\n        self.enc1 = self._make_layer(in_channels, base_filters)\n        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n\n        self.enc2 = self._make_layer(base_filters, base_filters*2)\n        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n\n        self.enc3 = self._make_layer(base_filters*2, base_filters*4)\n\n        self.upconv2 = nn.ConvTranspose3d(base_filters*4, base_filters*2, kernel_size=2, stride=2)\n        self.dec2 = self._make_layer(base_filters*4, base_filters*2)\n\n        self.upconv1 = nn.ConvTranspose3d(base_filters*2, base_filters, kernel_size=2, stride=2)\n        self.dec1 = self._make_layer(base_filters*2, base_filters)\n\n        self.dropout = nn.Dropout3d(0.10)\n        self.final_conv = nn.Conv3d(base_filters, out_channels, kernel_size=1)\n\n        self._initialize_weights()\n\n    def _make_layer(self, in_channels, out_channels):\n        \"\"\"Create a residual block\"\"\"\n        return nn.Sequential(\n            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.InstanceNorm3d(out_channels),\n            nn.LeakyReLU(inplace=True),\n            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.InstanceNorm3d(out_channels),\n            nn.LeakyReLU(inplace=True)\n        )\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.InstanceNorm3d):\n                if m.weight is not None:\n                    nn.init.constant_(m.weight, 1)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        enc1_out = self.enc1(x)\n        p1 = self.pool1(enc1_out)\n\n        enc2_out = self.enc2(p1)\n        p2 = self.pool2(enc2_out)\n\n        enc3_out = self.enc3(p2)\n\n        up2 = self.upconv2(enc3_out)\n\n        diffY = enc2_out.size()[2] - up2.size()[2]\n        diffX = enc2_out.size()[3] - up2.size()[3]\n        diffZ = enc2_out.size()[4] - up2.size()[4]\n\n        up2 = F.pad(up2, [\n            diffZ // 2, diffZ - diffZ // 2,\n            diffX // 2, diffX - diffX // 2,\n            diffY // 2, diffY - diffY // 2\n        ])\n        concat2 = torch.cat([up2, enc2_out], dim=1)\n        dec2_out = self.dec2(concat2)\n\n        up1 = self.upconv1(dec2_out)\n\n        diffY = enc1_out.size()[2] - up1.size()[2]\n        diffX = enc1_out.size()[3] - up1.size()[3]\n        diffZ = enc1_out.size()[4] - up1.size()[4]\n\n        up1 = F.pad(up1, [\n            diffZ // 2, diffZ - diffZ // 2,\n            diffX // 2, diffX - diffX // 2,\n            diffY // 2, diffY - diffY // 2\n        ])\n        concat1 = torch.cat([up1, enc1_out], dim=1)\n        dec1_out = self.dec1(concat1)\n\n        x = self.dropout(dec1_out)\n\n        out = self.final_conv(x)\n\n        return out\n\nmodel = ImprovedUNet3D(base_filters=16)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total parameters: {total_params:,}\")\nsize_in_bytes = total_params * 4\nsize_in_mb = size_in_bytes / (1024 ** 2)\nprint(f\"Model size (inference): {size_in_mb:.2f} MB\")\n",
            "metadata": {
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:39:47.883686Z",
                    "iopub.execute_input": "2025-07-01T17:39:47.884390Z",
                    "iopub.status.idle": "2025-07-01T17:39:47.905825Z",
                    "shell.execute_reply.started": "2025-07-01T17:39:47.884366Z",
                    "shell.execute_reply": "2025-07-01T17:39:47.905220Z"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Total parameters: 340,596\nModel size (inference): 1.30 MB\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 18
        },
        {
            "cell_type": "markdown",
            "source": "##Â Dataset processing",
            "metadata": {
                "id": "KG92aYgJNSmx"
            }
        },
        {
            "cell_type": "code",
            "source": "class BrainTumorDataset(Dataset):\n    def __init__(self, patient_list, data_dir, is_train=True, transform=None, slice_range=(60, 100)):\n        self.data_dir = data_dir              # Path to directory containing patient subfolders\n        self.is_train = is_train              # Flag indicating training or inference phase\n        self.transform = transform            # Optional data augmentations or preprocessing\n        # List of MRI modalities\n        self.modalities = [\"flair\", \"t1\", \"t1ce\", \"t2\"]\n\n        # Filter out any CSV files and exclude problematic patient 355\n        self.patients = [p for p in patient_list if not p.endswith(\".csv\") and \"355\" not in p]\n\n        # Range of axial slices to load (start_slice, end_slice)\n        self.slice_range = slice_range       \n\n    def load_nifti(self, path):\n        # Load NIfTI file from disk using nibabel and return as numpy float32 array\n        nifti_img = nib.load(path)\n        return np.array(nifti_img.get_fdata(), dtype=np.float32)\n\n    def __len__(self):\n        return len(self.patients)\n\n    def __getitem__(self, idx):\n        patient_id = self.patients[idx]\n\n        # Safeguard: skip patient 355 if encountered\n        if patient_id == \"BraTS20_Training_355\":\n            return None\n        # Construct path to this patient's data folder\n        patient_path = os.path.join(self.data_dir, patient_id)\n        # Load and preprocess each MRI modality\n        images = []\n        for modality in self.modalities:\n            img_path = os.path.join(patient_path, f\"{patient_id}_{modality}.nii\")\n            img = self.load_nifti(img_path)\n            # Crop to the specified slice range along the axial (third) dimension\n            start_slice, end_slice = self.slice_range\n            img = img[:, :, start_slice:end_slice]\n            # Apply z-score normalization for stability; avoid division by zero\n            if np.std(img) > 0:\n                img = (img - np.mean(img)) / np.std(img)\n            else:\n                img = np.zeros_like(img)\n\n            images.append(img)\n        # Stack the four modalities into a single tensor of shape (4, H, W, D)\n        images = np.stack(images, axis=0)\n        # Load segmentation mask if in training mode\n        if self.is_train:\n            # Possible filenames for segmentation masks\n            seg_paths = [\n                os.path.join(patient_path, f\"{patient_id}_seg.nii\"),\n                os.path.join(patient_path, f\"{patient_id}_Segm.nii\"),\n                os.path.join(patient_path, f\"{patient_id}_Segmentation.nii\")\n            ]\n            # Select the first existing segmentation file\n            seg_path = next((path for path in seg_paths if os.path.exists(path)), None)\n\n            if seg_path is None:\n                # Issue warning and create empty mask if not found\n                print(f\"Warning: No segmentation file found for patient {patient_id}.\")\n                mask = np.zeros_like(images[0], dtype=np.int64)\n            else:\n                # Load and crop mask to the same slice range\n                mask = self.load_nifti(seg_path)\n                mask = mask[:, :, self.slice_range[0]:self.slice_range[1]]\n                # Reassign label '4' (if present) to 3 for enhancing tumor\n                mask[mask == 4] = 3\n        else:\n            mask = None  # No mask needed for inference\n        # Apply optional transformations (e.g., augmentation) before tensor conversion\n        if self.transform is not None:\n            if self.is_train:\n                # When training, transform both image and mask\n                transformed = self.transform(image=images, mask=mask)\n                images, mask = transformed['image'], transformed['mask']\n            else:\n                # During inference, only transform the image\n                transformed = self.transform(image=images)\n                images = transformed['image']\n        # Convert numpy arrays to PyTorch tensors\n        if not isinstance(images, torch.Tensor):\n            images = torch.tensor(images, dtype=torch.float32)\n        if self.is_train and mask is not None:\n            # Convert mask to long tensor for use in loss functions\n            if not isinstance(mask, torch.Tensor):\n                mask = torch.tensor(mask, dtype=torch.long)\n            else:\n                mask = mask.to(dtype=torch.long)\n\n        # Return (image, mask) tuple for training or image only for inference\n        return (images, mask) if self.is_train else images",
            "metadata": {
                "id": "aksbOCQKj-bc",
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:33:48.983836Z",
                    "iopub.execute_input": "2025-07-01T17:33:48.984105Z",
                    "iopub.status.idle": "2025-07-01T17:33:48.995063Z",
                    "shell.execute_reply.started": "2025-07-01T17:33:48.984086Z",
                    "shell.execute_reply": "2025-07-01T17:33:48.994305Z"
                }
            },
            "outputs": [],
            "execution_count": 4
        },
        {
            "cell_type": "markdown",
            "source": "### Download the dataset",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "path = kagglehub.dataset_download(\"awsaf49/brats20-dataset-training-validation\")\ndata_dir = path + \"/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\ntrain_dir = path + \"/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\nval_dir = path +\"/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData\"",
            "metadata": {
                "id": "IF9IJJnyN2Nh",
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:33:52.716865Z",
                    "iopub.execute_input": "2025-07-01T17:33:52.717130Z",
                    "iopub.status.idle": "2025-07-01T17:33:52.838899Z",
                    "shell.execute_reply.started": "2025-07-01T17:33:52.717110Z",
                    "shell.execute_reply": "2025-07-01T17:33:52.838339Z"
                }
            },
            "outputs": [],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": "### visualization of the dataset",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "def visualize_samples_with_colors(data_dir, BraTS20_Training_00n):\n    flair = data_dir+ BraTS20_Training_00n + BraTS20_Training_00n +'_flair.nii'\n    mask = data_dir+ BraTS20_Training_00n + BraTS20_Training_00n +'_seg.nii'\n    t1 = data_dir+ BraTS20_Training_00n + BraTS20_Training_00n +'_t1.nii'\n    t2 = data_dir+ BraTS20_Training_00n + BraTS20_Training_00n +'_t2.nii'\n    t1ce = data_dir+ BraTS20_Training_00n + BraTS20_Training_00n +'_t1ce.nii'\n\n    flair_img = nib.load(flair)\n    flair_img = np.asanyarray(flair_img.dataobj)\n    flair_img = np.rot90(flair_img)\n\n    sample_mask = nib.load(mask)\n    sample_mask = np.asanyarray(sample_mask.dataobj)\n    sample_mask = np.rot90(sample_mask)\n\n    t1_img = nib.load(t1)\n    t1_img = np.asanyarray(t1_img.dataobj)\n    t1_img  = np.rot90(t1_img)\n\n    t2_img = nib.load(t2)\n    t2_img = np.asanyarray(t2_img.dataobj)\n    t2_img  = np.rot90(t2_img)\n\n    t1ce_img = nib.load(t1ce)\n    t1ce_img = np.asanyarray(t1ce_img.dataobj)\n    t1ce_img  = np.rot90(t1ce_img)\n\n    mask_WT = sample_mask.copy()\n    mask_WT[mask_WT == 1] = 1\n    mask_WT[mask_WT == 2] = 1\n    mask_WT[mask_WT == 4] = 1\n\n    mask_TC = sample_mask.copy()\n    mask_TC[mask_TC == 1] = 1\n    mask_TC[mask_TC == 2] = 0\n    mask_TC[mask_TC == 4] = 1\n\n    mask_ET = sample_mask.copy()\n    mask_ET[mask_ET == 1] = 0\n    mask_ET[mask_ET == 2] = 0\n    mask_ET[mask_ET == 4] = 1\n\n    fig = plt.figure(figsize=(20, 10))\n\n    gs = gridspec.GridSpec(nrows=2, ncols=5, height_ratios=[1, 1.5])\n\n    fontsize = 10\n    ax0 = fig.add_subplot(gs[0, 0])\n    flair = ax0.imshow(flair_img[:,:,65], cmap='bone')\n    ax0.set_title(\"FLAIR\", fontsize = fontsize, weight='bold', y=-0.2)\n    fig.colorbar(flair)\n\n    ax1 = fig.add_subplot(gs[0, 1])\n    t1 = ax1.imshow(t1_img[:,:,65], cmap='bone')\n    ax1.set_title(\"T1\", fontsize = fontsize, weight='bold', y=-0.2)\n    fig.colorbar(t1)\n    \n    ax2 = fig.add_subplot(gs[0, 2])\n    t2 = ax2.imshow(t2_img[:,:,65], cmap='bone')\n    ax2.set_title(\"T2\", fontsize = fontsize, weight='bold', y=-0.2)\n    fig.colorbar(t2)\n\n    ax3 = fig.add_subplot(gs[0, 3])\n    t1ce = ax3.imshow(t1ce_img[:,:,65], cmap='bone')\n    ax3.set_title(\"T1 contrast\", fontsize = fontsize, weight='bold', y=-0.2)\n    fig.colorbar(t1ce)\n\n    ax4 = fig.add_subplot(gs[1, 1:3])\n\n    l1 = ax4.imshow(mask_WT[:,:,65], cmap='summer',)\n    l2 = ax4.imshow(np.ma.masked_where(mask_TC[:,:,65]== False,  mask_TC[:,:,65]), cmap='rainbow', alpha=0.6)\n    l3 = ax4.imshow(np.ma.masked_where(mask_ET[:,:,65] == False, mask_ET[:,:,65]), cmap='winter', alpha=0.6)\n\n    ax4.set_title(\"\", fontsize=fontsize, weight='bold', y=-0.1)\n\n    _ = [ax.set_axis_off() for ax in [ax0,ax1,ax2,ax3, ax4]]\n\n    colors = [im.cmap(im.norm(1)) for im in [l1,l2, l3]]\n    labels = ['Non-Enhancing tumor core', 'Peritumoral Edema ', 'GD-enhancing tumor']\n    patches = [ mpatches.Patch(color=colors[i], label=f\"{labels[i]}\") for i in range(len(labels))]\n    plt.legend(handles=patches, bbox_to_anchor=(1.1, 0.65), loc=2, borderaxespad=0.4,fontsize = 'xx-large',\n              title='Mask Labels', title_fontsize=18, edgecolor=\"black\",  facecolor='#c5c6c7')\n\n    plt.suptitle(\"Multimodal Scans -  Data | Manually-segmented mask - Target\", fontsize=20, weight='bold')\n\n    fig.savefig(\"data_sample.png\", format=\"png\",  pad_inches=0.2, transparent=False, bbox_inches='tight')\n    fig.savefig(\"data_sample.svg\", format=\"svg\",  pad_inches=0.2, transparent=False, bbox_inches='tight')",
            "metadata": {
                "id": "kKhctvIIUz4l",
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:33:53.665326Z",
                    "iopub.execute_input": "2025-07-01T17:33:53.665570Z",
                    "iopub.status.idle": "2025-07-01T17:33:53.677947Z",
                    "shell.execute_reply.started": "2025-07-01T17:33:53.665551Z",
                    "shell.execute_reply": "2025-07-01T17:33:53.677307Z"
                }
            },
            "outputs": [],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": "## Dataset splitting and processing\n",
            "metadata": {
                "id": "4A-UpenYn-I6"
            }
        },
        {
            "cell_type": "code",
            "source": "train_patient_dirs = os.listdir(train_dir)\ntest_patient_dirs = os.listdir(val_dir)\nall_train_patients = []\nfor p in train_patient_dirs:\n    if os.path.isdir(os.path.join(train_dir, p)):\n        all_train_patients.append(p)\nall_train_patients.sort()\n\nall_test_patients = []\nfor p in test_patient_dirs:\n    if os.path.isdir(os.path.join(val_dir, p)):\n        all_test_patients.append(p)\nall_test_patients.sort()\n\n\nslice_range = (0, 155)\nslice_range = (0, 155)\ntrain_patients, val_patients = train_test_split(all_train_patients, test_size=0.2, random_state=42)\ntrain_dataset = BrainTumorDataset(\n    patient_list=train_patients,\n    data_dir=train_dir,\n    slice_range=slice_range,\n    transform=None\n)\n\nval_dataset = BrainTumorDataset(\n    patient_list=val_patients,\n    data_dir=train_dir,\n    slice_range=slice_range\n)\n\ntest_dataset = BrainTumorDataset(\n    patient_list=all_test_patients,\n    data_dir=val_dir,\n    is_train=False,\n    slice_range=slice_range\n)\nprint(f\"Using {len(train_dataset)} training patients and {len(val_dataset)} test patients\")\n\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)\n\nsample_img, sample_mask = next(iter(train_loader))\nprint(f\"Train set image dimension: {sample_img.shape}\")  # Should be smaller now\nprint(f\"Train set segmentation dimension: {sample_mask.shape}\")\n\nsample_img, sample_mask = next(iter(val_loader))\nprint(\"Validation set image dimension:\", sample_img.shape)\nprint(\"Validation set segmentation dimension:\", sample_mask.shape)\n\nsample_img = next(iter(test_loader))\nprint(\"Test set image dimension:\", sample_img.shape)",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "tW9R8xsgn5pq",
                "outputId": "9d6fa6f1-2932-4d46-b1a0-96ea1bcb4f33",
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:33:55.686418Z",
                    "iopub.execute_input": "2025-07-01T17:33:55.687230Z",
                    "iopub.status.idle": "2025-07-01T17:34:14.348263Z",
                    "shell.execute_reply.started": "2025-07-01T17:33:55.687205Z",
                    "shell.execute_reply": "2025-07-01T17:34:14.347300Z"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Using 294 training patients and 74 test patients\nTrain set image dimension: torch.Size([2, 4, 240, 240, 155])\nTrain set segmentation dimension: torch.Size([2, 240, 240, 155])\nValidation set image dimension: torch.Size([1, 4, 240, 240, 155])\nValidation set segmentation dimension: torch.Size([1, 240, 240, 155])\nTest set image dimension: torch.Size([1, 4, 240, 240, 155])\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "markdown",
            "source": "### set devices",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    gc.collect()\n    for i in range(5): \n        gc.collect()\n        torch.cuda.empty_cache()\n\n    num_gpus = torch.cuda.device_count()\n    print(f\"Using {num_gpus} GPUs for training.\" if num_gpus > 1 else\n          f\"Using {torch.cuda.get_device_name(0)} for training.\")\n\n    # Print available and total memory\n    if hasattr(torch.cuda, 'get_device_properties'):\n        for i in range(num_gpus):\n            prop = torch.cuda.get_device_properties(i)\n            print(f\"GPU {i}: {prop.name}\")\n            print(f\"  Total memory: {prop.total_memory / 1024**3:.2f} GB\")\n            print(f\"  Available memory: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB reserved\")\n            print(f\"  Allocated memory: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\nelse:\n    print(\"Using CPU for training.\")",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "5EL269D_OdDi",
                "outputId": "90c6ecb2-a785-4144-ccb0-8a7cae6cbd30",
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:34:42.241872Z",
                    "iopub.execute_input": "2025-07-01T17:34:42.242596Z",
                    "iopub.status.idle": "2025-07-01T17:34:43.993052Z",
                    "shell.execute_reply.started": "2025-07-01T17:34:42.242567Z",
                    "shell.execute_reply": "2025-07-01T17:34:43.992397Z"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Using 2 GPUs for training.\nGPU 0: Tesla T4\n  Total memory: 14.74 GB\n  Available memory: 0.00 GB reserved\n  Allocated memory: 0.00 GB\nGPU 1: Tesla T4\n  Total memory: 14.74 GB\n  Available memory: 0.00 GB reserved\n  Allocated memory: 0.00 GB\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": "def dice_coefficient_soft(pred_probs, target, num_classes=4, smooth=1e-6):\n    dice_scores = []\n    target_one_hot = F.one_hot(target, num_classes=num_classes).permute(0, 4, 1, 2, 3).float()\n    for class_id in range(num_classes):\n        pred_class = pred_probs[:, class_id, ...]\n        target_class = target_one_hot[:, class_id, ...]\n        intersection = (pred_class * target_class).sum(dim=[1, 2, 3]) # Sum over spatial dims\n        pred_sum = pred_class.sum(dim=[1, 2, 3])\n        target_sum = target_class.sum(dim=[1, 2, 3])\n        dice_score_class = (2. * intersection + smooth) / (pred_sum + target_sum + smooth)\n        dice_scores.append(dice_score_class.mean())\n    dice_scores = torch.stack(dice_scores)\n    return dice_scores.mean() \n\n# Modified DiceLoss using Soft Dice\nclass DiceLoss(nn.Module):\n    def __init__(self, num_classes=4, smooth=1e-6):\n        super(DiceLoss, self).__init__()\n        self.num_classes = num_classes\n        self.smooth = smooth\n\n    def forward(self, y_pred_logits, y_true):\n        y_pred_probs = F.softmax(y_pred_logits, dim=1)\n        dice = dice_coefficient_soft(y_pred_probs, y_true, num_classes=self.num_classes, smooth=self.smooth)\n        return 1 - dice\n\nclass CombinedLoss(nn.Module):\n    def __init__(self, weight_dice=0.7, weight_ce=0.3, num_classes=4):\n        super(CombinedLoss, self).__init__()\n        self.weight_dice = weight_dice\n        self.weight_ce = weight_ce\n        self.dice_loss = DiceLoss(num_classes=num_classes)\n        self.ce_loss = nn.CrossEntropyLoss()\n\n    def forward(self, inputs_logits, targets):\n        ce_loss_value = self.ce_loss(inputs_logits, targets)\n        dice_loss_value = self.dice_loss(inputs_logits, targets)\n        return self.weight_ce * ce_loss_value + self.weight_dice * dice_loss_value\n\ndef dice_coefficient_metric(pred_labels, target_labels, num_classes=4, smooth=1e-6):\n    dice_scores = []\n    for class_id in range(num_classes):\n        pred_class = (pred_labels == class_id).float()\n        target_class = (target_labels == class_id).float()\n        intersection = (pred_class * target_class).sum()\n        dice_score = (2. * intersection + smooth) / (pred_class.sum() + target_class.sum() + smooth)\n        dice_scores.append(dice_score)\n    dice_scores = torch.stack(dice_scores)\n    return dice_scores.mean()",
            "metadata": {
                "id": "BArIB62TSksv",
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:34:43.994003Z",
                    "iopub.execute_input": "2025-07-01T17:34:43.994321Z",
                    "iopub.status.idle": "2025-07-01T17:34:44.003586Z",
                    "shell.execute_reply.started": "2025-07-01T17:34:43.994303Z",
                    "shell.execute_reply": "2025-07-01T17:34:44.003081Z"
                }
            },
            "outputs": [],
            "execution_count": 9
        },
        {
            "cell_type": "markdown",
            "source": "## Create the model",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "model = ImprovedUNet3D(in_channels=4, out_channels=4, base_filters=16)\nmodel = torch.nn.DataParallel(model)\nmodel.to(device)\n# Use mixed precision for memory efficiency\nuse_amp = True\nscaler = torch.amp.GradScaler(\"cuda\", enabled=\"use_amp\")\ncriterion = CombinedLoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=5e-3)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)",
            "metadata": {
                "id": "OGQYkGhUSjM5",
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:40:01.896658Z",
                    "iopub.execute_input": "2025-07-01T17:40:01.896968Z",
                    "iopub.status.idle": "2025-07-01T17:40:01.913482Z",
                    "shell.execute_reply.started": "2025-07-01T17:40:01.896946Z",
                    "shell.execute_reply": "2025-07-01T17:40:01.913004Z"
                }
            },
            "outputs": [],
            "execution_count": 19
        },
        {
            "cell_type": "code",
            "source": "import torch\n\ndef train_one_epoch(model, dataloader, optimizer, criterion, device, scaler=None):\n    \"\"\"\n    Train the model for one epoch.\n    Args:\n        model: PyTorch model to train.\n        dataloader: DataLoader providing (inputs, targets).\n        optimizer: Optimizer for parameter updates.\n        criterion: Loss function to compute training loss.\n        device: Torch device (cpu or cuda).\n        scaler: Optional GradScaler for mixed precision.\n    Returns:\n        avg_loss: Average training loss over batches.\n        avg_dice: Average Dice score over batches.\n        avg_dice_per_class: Tensor of average Dice per class.\n    \"\"\"\n    use_amp = scaler is not None  # Flag whether to use automatic mixed precision\n    model.train()                 # Set model to training mode\n    epoch_loss = 0.0              # Cumulative loss for the epoch\n    dice_score = 0.0              # Cumulative Dice score\n    dice_per_class = torch.zeros(4, device=device)  # Sum of per-class Dice scores\n    batch_count = 0               # Number of processed batches\n\n    for i, (inputs, targets) in enumerate(dataloader):\n        # Move data to target device\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n        optimizer.zero_grad()      # Reset gradients\n\n        if use_amp:\n            # Use mixed precision context\n            with torch.cuda.amp.autocast(enabled=True):\n                outputs = model(inputs)             # Forward pass\n                loss = criterion(outputs, targets)  # Compute loss\n            # Scale loss, backpropagate, and unscale gradients\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n            # Gradient clipping for stability\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.5)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            outputs = model(inputs)                 # Forward pass\n            loss = criterion(outputs, targets)      # Compute loss\n            loss.backward()                         # Backpropagate\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.5)\n            optimizer.step()                        # Update parameters\n\n        epoch_loss += loss.item()  # Accumulate loss\n\n        # Generate predictions: handle multi-class vs binary\n        if outputs.shape[1] > 1:\n            _, preds = torch.max(outputs, dim=1)\n        else:\n            preds = (torch.sigmoid(outputs) > 0.5).float()\n\n        # Compute overall Dice score for the batch\n        dice = dice_coefficient_metric(preds, targets)\n        dice_score += dice.item()\n\n        # Compute per-class Dice scores\n        class_dice_scores = []\n        for class_id in range(4):\n            pred_class = (preds == class_id).float()\n            target_class = (targets == class_id).float()\n            intersection = (pred_class * target_class).sum()\n            class_dice = (2. * intersection + 1e-6) / (pred_class.sum() + target_class.sum() + 1e-6)\n            dice_per_class[class_id] += class_dice.item()\n            class_dice_scores.append(class_dice.item())\n\n        batch_count += 1\n        # Print progress every 5 batches\n        if (i + 1) % 5 == 0:\n            print(f\"Batch {i+1}/{len(dataloader)}, Loss: {loss.item():.4f}, Dice: {dice:.4f}\")\n            print(f\"  Class Dice: Background: {class_dice_scores[0]:.4f}, \\\n                  NCR: {class_dice_scores[1]:.4f}, ED: {class_dice_scores[2]:.4f}, ET: {class_dice_scores[3]:.4f}\")\n\n        # Free memory\n        del inputs, targets, outputs, preds\n        torch.cuda.empty_cache()\n\n    # Compute average metrics over all batches\n    avg_loss = epoch_loss / batch_count\n    avg_dice = dice_score / batch_count\n    avg_dice_per_class = dice_per_class / batch_count\n\n    return avg_loss, avg_dice, avg_dice_per_class\n\n\ndef validate_one_epoch(model, dataloader, criterion, device, scaler=None):\n    \"\"\"\n    Validate the model for one epoch without gradient updates.\n    Args:\n        model: PyTorch model to evaluate.\n        dataloader: DataLoader providing (inputs, targets).\n        criterion: Loss function to compute validation loss.\n        device: Torch device (cpu or cuda).\n        scaler: Optional GradScaler for mixed precision during validation.\n    Returns:\n        avg_loss: Average validation loss.\n        avg_dice: Average Dice score.\n        avg_dice_per_class: Tensor of average Dice per class.\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n    epoch_loss = 0.0\n    dice_score = 0.0\n    dice_per_class = torch.zeros(4, device=device)\n    batch_count = 0\n\n    with torch.no_grad():  # Disable gradient calculation\n        for i, (inputs, targets) in enumerate(dataloader):\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n\n            # Determine number of classes from criterion if available\n            if hasattr(criterion, 'dice_loss') and hasattr(criterion.dice_loss, 'num_classes'):\n                num_classes = criterion.dice_loss.num_classes\n            else:\n                num_classes = 4  # Fallback to 4 classes\n\n            # Forward pass under mixed precision if scaler provided\n            with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n                outputs = model(inputs)             # Logits from model\n                loss = criterion(outputs, targets)  # Loss computation\n\n            epoch_loss += loss.item()\n\n            # Get predictions for metric computation\n            if outputs.shape[1] > 1:\n                _, preds = torch.max(outputs, dim=1)\n            else:\n                preds = (torch.sigmoid(outputs) > 0.5).float()\n\n            # Compute overall Dice score\n            dice = dice_coefficient_metric(preds, targets, num_classes=num_classes)\n            dice_score += dice.item()\n\n            # Accumulate per-class Dice scores\n            for class_id in range(num_classes):\n                pred_class = (preds == class_id).float()\n                target_class = (targets == class_id).float()\n                intersection = (pred_class * target_class).sum()\n                class_dice = (2. * intersection + 1e-6) / (pred_class.sum() + target_class.sum() + 1e-6)\n                dice_per_class[class_id] += class_dice.item()\n\n            batch_count += 1\n            # Free memory\n            del inputs, targets, outputs, preds\n            torch.cuda.empty_cache()\n\n    # Average validation metrics\n    avg_loss = epoch_loss / batch_count\n    avg_dice = dice_score / batch_count\n    avg_dice_per_class = dice_per_class / batch_count\n\n    return avg_loss, avg_dice, avg_dice_per_class\n\n\nclass EarlyStopping:\n    \"\"\"\n    Early stops the training if validation loss doesn't improve after a given patience.\n    \"\"\"\n    def __init__(self, patience=7, min_delta=0.001):\n        self.patience = patience      # Number of epochs to wait after last improvement\n        self.min_delta = min_delta    # Minimum change to qualify as improvement\n        self.counter = 0              # Counter for epochs without improvement\n        self.best_score = None        # Best observed validation loss\n        self.early_stop = False       # Flag to indicate whether to stop training\n\n    def __call__(self, val_loss):\n        # Initialize best_score at first call\n        if self.best_score is None:\n            self.best_score = val_loss\n        # If no significant improvement, increment counter\n        elif val_loss > self.best_score + self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True  # Trigger early stop\n        else:\n            # Improvement observed: reset counter and update best_score\n            self.best_score = val_loss\n            self.counter = 0",
            "metadata": {
                "id": "tqH7BNSlSZiZ",
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:34:55.529246Z",
                    "iopub.execute_input": "2025-07-01T17:34:55.530420Z",
                    "iopub.status.idle": "2025-07-01T17:34:55.549087Z",
                    "shell.execute_reply.started": "2025-07-01T17:34:55.530388Z",
                    "shell.execute_reply": "2025-07-01T17:34:55.548288Z"
                }
            },
            "outputs": [],
            "execution_count": 11
        },
        {
            "cell_type": "code",
            "source": "user_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\napi = HfApi()\nrepo_name = \"dugoalberto/3D_U_Net\"\ncheckpoint_dir = \"./checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)",
            "metadata": {
                "id": "FNKNIiELT26M",
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:34:57.921400Z",
                    "iopub.execute_input": "2025-07-01T17:34:57.921878Z",
                    "iopub.status.idle": "2025-07-01T17:34:58.057073Z",
                    "shell.execute_reply.started": "2025-07-01T17:34:57.921856Z",
                    "shell.execute_reply": "2025-07-01T17:34:58.056346Z"
                }
            },
            "outputs": [],
            "execution_count": 12
        },
        {
            "cell_type": "markdown",
            "source": "## Training (from new model)",
            "metadata": {
                "id": "ds5nmMtnUw-G"
            }
        },
        {
            "cell_type": "code",
            "source": "# Training loop with early stopping\nnum_epochs = 300  # Increased epochs with early stopping\nbest_val_dice = 0.0\nearly_stopping = EarlyStopping(patience=10)\n\n# Lists to store metrics\ntrain_losses = []\nval_losses = []\ntrain_dice_scores = []\nval_dice_scores = []\nlearning_rates = []\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n\n    # Train\n    train_loss, train_dice = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n    train_losses.append(train_loss)\n    train_dice_scores.append(train_dice)\n\n    # Update learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    learning_rates.append(current_lr)\n    if scheduler is not None and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            scheduler.step(val_loss)\n    else: \n        scheduler.step()\n\n    # Free memory before validation\n    torch.cuda.empty_cache()\n\n    # Validate\n    val_loss, val_dice = validate_one_epoch(model, val_loader, criterion, device)\n    val_losses.append(val_loss)\n    val_dice_scores.append(val_dice)\n\n    # Print epoch results\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f}, Validation Dice: {val_dice:.4f}, LR: {current_lr:.6f}\")\n\n    if val_dice > best_val_dice:\n        best_val_dice = val_dice\n        best_model_path = os.path.join(checkpoint_dir, 'BratsCV_best.pt')\n        torch.save(model, best_model_path)\n        print(f\"Best model saved with Dice score: {best_val_dice:.4f}!\")\n\n        # Upload best model to Hugging Face\n        api.upload_file(\n            path_or_fileobj=best_model_path,\n            path_in_repo=\"BratsCV_best.pt\",\n            repo_id=repo_name,\n            token=hf_token\n        )\n        print(f\"Best model uploaded to Hugging Face!\")\n\n    # Save regular checkpoint every 5 epochs or final epoch\n    if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        checkpoint_path = os.path.join(checkpoint_dir, f'BratsCV_epoch_{epoch+1}_{timestamp}.pt')\n\n        # Save model state dictionary and optimizer state\n        checkpoint = {\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n            'train_dice': train_dice,\n            'val_dice': val_dice,\n            'best_val_dice': best_val_dice,\n            'learning_rate': current_lr\n        }\n        torch.save(checkpoint, checkpoint_path)\n\n        # Upload checkpoint to Hugging Face\n        try:\n            api.upload_file(\n                path_or_fileobj=checkpoint_path,\n                path_in_repo=f\"checkpoints/BratsCV_epoch_{epoch+1}.pt\",\n                repo_id=repo_name,\n                token=hf_token\n            )\n            print(f\"Checkpoint for epoch {epoch+1} uploaded to Hugging Face!\")\n        except Exception as e:\n            print(f\"Error uploading checkpoint to Hugging Face: {e}\")\n\n    early_stopping(val_loss)\n    if early_stopping.early_stop:\n        print(f\"Early stopping triggered after {epoch+1} epochs!\")\n        final_model_path = os.path.join(checkpoint_dir, 'BratsCV_final.pt')\n        torch.save(model, final_model_path)\n        api.upload_file(\n            path_or_fileobj=final_model_path,\n            path_in_repo=\"BratsCV_final.pt\",\n            repo_id=repo_name,\n            token=hf_token\n        )\n        print(\"Final model uploaded to Hugging Face!\")\n        break\n\n    # Free memory after each epoch\n    torch.cuda.empty_cache()\n\n# Plot training and validation metrics\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.title('Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 3, 2)\nplt.plot(train_dice_scores, label='Train Dice')\nplt.plot(val_dice_scores, label='Validation Dice')\nplt.title('Dice Score')\nplt.xlabel('Epoch')\nplt.ylabel('Dice')\nplt.legend()\n\nplt.subplot(1, 3, 3)\nplt.plot(learning_rates)\nplt.title('Learning Rate')\nplt.xlabel('Epoch')\nplt.ylabel('LR')\n\nplt.tight_layout()\nplt.savefig('training_metrics.png')\nplt.show()\n\nprint(f\"Training completed! Best validation Dice score: {best_val_dice:.4f}\")",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 442
                },
                "id": "tvBKokUoSFQc",
                "outputId": "24f46f40-bb20-4cf6-c376-cb7260f88118",
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        },
        {
            "cell_type": "markdown",
            "source": "## Training (from restored model)",
            "metadata": {
                "id": "A9GdedRxU8lv"
            }
        },
        {
            "cell_type": "code",
            "source": "# Nome del checkpoint da recuperare\ncheckpoint_filename = \"checkpoints/BratsCV_epoch_120.pt\"\nrepo_name = \"dugoalberto/3D_U_Net\"\nlocal_checkpoint_path = api.hf_hub_download(\n        repo_id=repo_name,\n        filename=checkpoint_filename,\n        token=hf_token\n)\ncheckpoint = torch.load(local_checkpoint_path)\n\n# Verifica i parametri recuperati\nprint(\"Checkpoint Recovery Information:\")\nprint(\"-\" * 40)\n\n# Stampa informazioni sull'epoch\nprint(f\"Epoch: {checkpoint.get('epoch', 'Not found')}\")\n\n# Stampa metriche\nprint(f\"Training Loss: {checkpoint.get('train_loss', 'Not found')}\")\nprint(f\"Validation Loss: {checkpoint.get('val_loss', 'Not found')}\")\nprint(f\"Training Dice Score: {checkpoint.get('train_dice', 'Not found')}\")\nprint(f\"Validation Dice Score: {checkpoint.get('val_dice', 'Not found')}\")\nprint(f\"Best Validation Dice Score: {checkpoint.get('best_val_dice', 'Not found')}\")\n\nmodel = ImprovedUNet3D()\nmodel = torch.nn.DataParallel(model)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.to(device)",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 188,
                    "referenced_widgets": [
                        "4d33e2acac44408bbaec22cd12087d4b",
                        "c547671ecdee43538c4e0b125169a84f",
                        "ee904822cfb4460db7725590b50992b9",
                        "3a85e48858b04398bd73c770f2cd79ef",
                        "6ec5667f949546f19e938b801dc6454e",
                        "9d7b2a9c3b8c4e6597ace70ca37faa0f",
                        "8d37c15c27b14da69d99a11abb5632a0",
                        "84f93c59f03a45bfadf277cdb4cd6226",
                        "1485ba524e2d48cc95f0872593022113",
                        "b316a62934844a6dac2d14796ae781f4",
                        "79940d98186349e7b1a2b6164c059c82"
                    ]
                },
                "id": "3tWiI6eHYZDg",
                "outputId": "76852b2a-b960-4d72-8622-cbdb4f664346",
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:40:42.051036Z",
                    "iopub.execute_input": "2025-07-01T17:40:42.051317Z",
                    "iopub.status.idle": "2025-07-01T17:40:42.162487Z",
                    "shell.execute_reply.started": "2025-07-01T17:40:42.051297Z",
                    "shell.execute_reply": "2025-07-01T17:40:42.161746Z"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Checkpoint Recovery Information:\n----------------------------------------\nEpoch: 120\nTraining Loss: 0.19613416381433707\nValidation Loss: 0.2129683260982101\nTraining Dice Score: 0.8485563746114977\nValidation Dice Score: 0.787887025121096\nBest Validation Dice Score: 0.7913068532943726\n",
                    "output_type": "stream"
                },
                {
                    "name": "stderr",
                    "text": "/tmp/ipykernel_31/3430020675.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(local_checkpoint_path)\n",
                    "output_type": "stream"
                },
                {
                    "execution_count": 22,
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": "DataParallel(\n  (module): ImprovedUNet3D(\n    (enc1): Sequential(\n      (0): Conv3d(4, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n      (3): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (4): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    )\n    (pool1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (enc2): Sequential(\n      (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n      (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (4): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    )\n    (pool2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (enc3): Sequential(\n      (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n      (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (4): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    )\n    (upconv2): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n    (dec2): Sequential(\n      (0): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n      (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (4): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    )\n    (upconv1): ConvTranspose3d(32, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n    (dec1): Sequential(\n      (0): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n      (3): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n      (4): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n      (5): LeakyReLU(negative_slope=0.01, inplace=True)\n    )\n    (dropout): Dropout3d(p=0.1, inplace=False)\n    (final_conv): Conv3d(16, 4, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  )\n)"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 22
        },
        {
            "cell_type": "code",
            "source": "optimizer = optim.AdamW(model.parameters(), lr=0.00004, weight_decay=5e-6)\n#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='max',           # 'max' because we want to maximize validation accuracy\n    factor=0.8,           # Multiply LR by this factor when reducing\n    patience=2,           # Number of epochs with no improvement after which LR will be reduced\n    verbose=True,         # Print message when LR is reduced\n    threshold=0.1,      # Threshold for measuring improvement\n    min_lr=1e-9           # Lower bound on the learning rate\n)\n\nnum_epochs = 300  # Increased epochs with early stopping\nstart_epoch = checkpoint['epoch']\nearly_stopping = EarlyStopping(patience=10)\nuse_amp = True\nscaler = torch.cuda.amp.GradScaler(enabled=use_amp)",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "2IGF9_-vU99F",
                "outputId": "99eec821-4f0d-49e6-b88c-b9f3b476ac31",
                "trusted": true,
                "execution": {
                    "iopub.status.busy": "2025-07-01T17:40:48.973219Z",
                    "iopub.execute_input": "2025-07-01T17:40:48.973495Z",
                    "iopub.status.idle": "2025-07-01T17:40:48.986269Z",
                    "shell.execute_reply.started": "2025-07-01T17:40:48.973474Z",
                    "shell.execute_reply": "2025-07-01T17:40:48.985514Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "text": "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n/tmp/ipykernel_31/603968276.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 23
        },
        {
            "cell_type": "code",
            "source": "# Lists to store metrics\ntrain_losses = []\nval_losses = []\ntrain_dice_scores = []\nval_dice_scores = []\nlearning_rates = []\n\nbest_val_dice = checkpoint['best_val_dice']\n\n# Restart the training\nfor epoch in range(start_epoch, num_epochs):\n    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n    # Train\n    train_loss, train_dice = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n    train_losses.append(train_loss)\n    train_dice_scores.append(train_dice)\n\n    # Free memory before validation\n    torch.cuda.empty_cache()\n\n    # Validate\n    val_loss, val_dice = validate_one_epoch(model, val_loader, criterion, device)\n    val_losses.append(val_loss)\n    val_dice_scores.append(val_dice)\n    \n    # Update learning rate\n    current_lr = optimizer.param_groups[0]['lr']\n    print(current_lr)\n    learning_rates.append(current_lr)\n    if scheduler is not None and isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            scheduler.step(val_loss)\n    else :\n        scheduler.step()\n    #torch.cuda.empty_cache()\n    # Print epoch results\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}\")\n    print(f\"Validation Loss: {val_loss:.4f}, Validation Dice: {val_dice:.4f}\")\n\n    if val_dice > best_val_dice:\n        best_val_dice = val_dice\n        best_model_path = os.path.join(checkpoint_dir, 'BratsCV_best.pt')\n        torch.save(model, best_model_path)\n        print(f\"Best model saved with Dice score: {best_val_dice:.4f}!\")\n        api.upload_file(\n            path_or_fileobj=best_model_path,\n            path_in_repo=\"BratsCV_best.pt\",\n            repo_id=repo_name,\n            token=hf_token\n        )\n        print(f\"Best model uploaded to Hugging Face!\")\n    if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        checkpoint_path = os.path.join(checkpoint_dir, f'BratsCV_epoch_{epoch+1}_{timestamp}.pt')\n\n        # Save model state dictionary and optimizer state\n        checkpoint = {\n            'epoch': epoch + 1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n            'train_dice': train_dice,\n            'val_dice': val_dice,\n            'best_val_dice': best_val_dice,\n            'learning_rate': current_lr\n        }\n        torch.save(checkpoint, checkpoint_path)\n\n        # Upload checkpoint to Hugging Face\n        try:\n            api.upload_file(\n                path_or_fileobj=checkpoint_path,\n                path_in_repo=f\"checkpoints/BratsCV_epoch_{epoch+1}.pt\",\n                repo_id=repo_name,\n                token=hf_token\n            )\n            print(f\"Checkpoint for epoch {epoch+1} uploaded to Hugging Face!\")\n        except Exception as e:\n            print(f\"Error uploading checkpoint to Hugging Face: {e}\")\n\n    # Check for early stopping\n    early_stopping(val_loss)\n    if early_stopping.early_stop:\n        print(f\"Early stopping triggered after {epoch+1} epochs!\")\n\n        # Save final model after early stopping\n        final_model_path = os.path.join(checkpoint_dir, 'BratsCV_final.pt')\n        torch.save(model, final_model_path)\n\n        # Upload final model to Hugging Face\n        api.upload_file(\n            path_or_fileobj=final_model_path,\n            path_in_repo=\"BratsCV_final.pt\",\n            repo_id=repo_name,\n            token=hf_token\n        )\n        print(\"Final model uploaded to Hugging Face!\")\n        break",
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000,
                    "referenced_widgets": [
                        "c1db8fd726df48d4a26bf2f7cab9ea33",
                        "3b8d6460771547d397adabb40db58016",
                        "6cd65914eed846e4a84c9e008d5354e2",
                        "fad750b9adb548369af2ec7d0b75d267",
                        "6cd2a21390c54dce826d5c0c1bdd90ca",
                        "39f6ea6f3d604ad195a7abf96142a672",
                        "7142cbf7d6f84abe94dc9467a2e20aee",
                        "423ea92d3f764b31b5cc9517a0b34494",
                        "f770dcc71e9d4263a1d25a0150869663",
                        "b10f2729cb0a458fa5ff2c7d859ab724",
                        "29332ea751b746a98098a4782ab837ef",
                        "35be443e8c3a4ea1a3211ee25da8f669",
                        "e0c787d4ca664b25a13b6bd06322aa4b",
                        "8038ac8f6df742868be37cdfce377620",
                        "e4fd195805604064950f941d03b9c50a",
                        "7751f31624d34e45b7656f4ecad497ba",
                        "955965f1854142f78ccd727edc0c36f5",
                        "aba02c8406f241fca6a0196757273ad7",
                        "4579438d457d4ae298fceef22c4f215e",
                        "66aff2997480405a8b1fcd89dfe8e649",
                        "c9ea97383ac845c1aa0c6b151262f4f0",
                        "7ff92b0a76814a68ad728b489acfed39",
                        "b4f78045b2e84f2f83beae7fc3aa3082",
                        "e47827be604c42b6b78a148ab663a09a",
                        "d64dd359c027474a98eefa022738bf92",
                        "d81836cff24e488d946f14b39258461a",
                        "76bb710b89e84c7a96a3aa413c516121",
                        "48a92b6ea58148eea8beac5634e68697",
                        "a7b46608184b46a2821b730845144cdd",
                        "b393678815784cc08989ea57d828c4b9",
                        "bbf566158f134821953f2054253cb9de",
                        "6dd43c5befe84bbabb535c93ffe9a98b",
                        "379cf7850a6d4b2c8a7b2b7f9e25295e",
                        "9152c09ce22242e199333fadab088c5e",
                        "e70fb89cf5bb40d7a801b9d8c6d10cc7",
                        "334ac583728545e4bb53c5907b03cab2",
                        "d15734cd3c434038b5779135ae20dae2",
                        "b4b8230f99a14d4894ff5dadedeba129",
                        "8a19294166874dd69ed87ab28deaf79b",
                        "759a9641cd234af282fd13a4c308b378",
                        "552269a2e190488b8c5ccbd111b27d2e",
                        "a6f5587506574aebb064896706609a9c",
                        "aa0e91ef105440218493c8683157abd4",
                        "ad80a1872a934874821b072973b95a4d",
                        "46b3e77478514273b87f91cd2e7acda6",
                        "e8aaac2b86034c29a52ed5f6c2071d13",
                        "c5fe4eeb165143fcbd692c7ca621b1e3",
                        "423134a8c93d44bfa19a10b16aac7078",
                        "ac678d71eafb41749553bccca9839954",
                        "85f3f262837749a0be0c36d8f6c01128",
                        "604cfe51c90b46a88b6b5d1c30db53d9",
                        "50fc3f2b3faf4d728e2add5c11cfc807",
                        "d2662596fb2740f597399556464db6ae",
                        "959fc1624d3f4bb5bbddde7fd4385b02",
                        "6dccbda6c7ac44778efdbac98b3e11c7"
                    ]
                },
                "id": "hJPAKLWNaAas",
                "outputId": "c041559e-fb87-419b-dbff-72dae22dcd66",
                "trusted": true
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}